{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression in Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import pystan\n",
    "\n",
    "from utils import count_divergences, compute_metrics, SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Fraud Detection [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%gcs read --object \"gs://thesis-203306/data/creditcard.csv\" --variable csv_as_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(BytesIO(csv_as_bytes))\n",
    "\n",
    "y = np.array(df.Class.tolist())     \n",
    "df = df.drop('Class', 1)\n",
    "df = df.drop('Time', 1)     \n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "\n",
    "X = np.array(df.values)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy dataset\n",
    "\n",
    "Adapted from [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D = 1, noise_std=0.1):    \n",
    "    X = np.concatenate([np.linspace(-6, -5, num=5), np.linspace(2, 6, num=N-5)])\n",
    "    y = np.tanh(X) + np.random.normal(0, noise_std, size=N)\n",
    "    y[y < 0.5] = 0\n",
    "    y[y >= 0.5] = 1\n",
    "    X = (X - 4.0) / 4.0\n",
    "    X = X.reshape((N, D))\n",
    "    return X, y.astype(int)\n",
    "  \n",
    "N1 = 100\n",
    "D1 = 1\n",
    "X1, y1 = build_toy_dataset(N1, D=1, noise_std=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the precompiled model\n",
    "sm = pickle.load(open('bay_log_reg.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively, compile the model\n",
    "\n",
    "# stan_code = \"\"\"\n",
    "# data {\n",
    "#   int<lower=0> N; \n",
    "#   int<lower=0> D; \n",
    "#   int<lower=0, upper=1> y[N];\n",
    "#   matrix[N,D] X; \n",
    "# }\n",
    "\n",
    "# parameters {\n",
    "#   vector[D] w; \n",
    "#   real b; \n",
    "# }\n",
    "\n",
    "# model {  \n",
    "#   w ~ normal(0, 3);\n",
    "#   b ~ normal(0, 3);\n",
    "#   for (n in 1:N)\n",
    "#       y[n] ~ bernoulli_logit(dot_product(X[n],w) + b);\n",
    "# }\n",
    "# \"\"\"\n",
    "# sm = pystan.StanModel(model_code=stan_code)\n",
    "\n",
    "# # Save model to file\n",
    "# with open('bay_log_reg.pkl', 'wb') as f:\n",
    "#     pickle.dump(sm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stan_nuts(X,y,filename,seeds=SEED):\n",
    "  \"\"\"\n",
    "  Runs Stan NUTS algorithm\n",
    "  Default: 1000 iterations (500 warmup)\n",
    "  \"\"\"\n",
    "  for seed in seeds:\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed)\n",
    "    N,D = X_train.shape    \n",
    "    stan_data = dict(N=N, D=D, X=X_train, y=y_train)\n",
    "    start = timer()\n",
    "    fit = sm.sampling(iter=1000, warmup=500, data=stan_data, chains=2, seed=seed)\n",
    "    end = timer()\n",
    "    e = fit.extract(permuted=False).mean(axis=1)\n",
    "    w = e[:,:D]\n",
    "    b = e[:,D]\n",
    "    \n",
    "    F1 = []\n",
    "    accuracy = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    for ww,bb in zip(w,b):\n",
    "      a, p, r, f = compute_metrics(ww,bb,X_test,y_test)\n",
    "      accuracy.append(a)\n",
    "      precision.append(p)\n",
    "      recall.append(r)\n",
    "      F1.append(f)\n",
    "\n",
    "    results = {'w': w, 'b': b, 'iters': 1000, 'warmup': 500, 'divergences': count_divergences(fit),\n",
    "              'accuracy': accuracy, 'precision': precision, 'recall': recall, 'F1': F1, 'time': end-start} \n",
    "    \n",
    "    with open('results/stan/{}_{}.pkl'.format(filename, seed), 'wb') as f:\n",
    "      pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# toy example\n",
    "stan_nuts(X1,y1,'toy_example',seeds=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# credit card\n",
    "stan_nuts(X,y,'nuts_credit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stan_vi(X, y, filename, seeds=SEED):\n",
    "  \"\"\"\n",
    "  Runs Stan ADVI algorithm\n",
    "  \"\"\"\n",
    "  for seed in seeds:\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed)\n",
    "    N,D = X_train.shape   \n",
    "    stan_data = dict(N=N, D=D, X=X_train, y=y_train)\n",
    "    \n",
    "    F1 = []\n",
    "    accuracy = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    times = []\n",
    "    \n",
    "    iters = np.linspace(500, 10000, 5).astype(int)\n",
    "    for it in iters:\n",
    "      start = timer()\n",
    "      fit = sm.vb(data=stan_data, algorithm='meanfield', iter=it,\n",
    "                              tol_rel_obj=0.0001, seed=seed, output_samples=1000)\n",
    "      end = timer()\n",
    "      \n",
    "      w = fit['mean_pars'][:D]\n",
    "      b = fit['mean_pars'][D]\n",
    "      \n",
    "      a, p, r, f = compute_metrics(w,b,X_test,y_test)\n",
    "      accuracy.append(a)\n",
    "      precision.append(p)\n",
    "      recall.append(r)\n",
    "      F1.append(f)\n",
    "      times.append(end-start)\n",
    "      \n",
    "      samples = {'w': np.array(fit['sampler_params'][:D]), 'b': np.array(fit['sampler_params'][D])}\n",
    "      with open('results/stan/{}_{}_samples.pkl'.format(filename, seed), 'ab') as f:\n",
    "        pickle.dump(samples, f)\n",
    "      \n",
    "    results = {'iters': iters, 'times': times, 'accuracy': accuracy, 'precision': precision,\n",
    "               'recall': recall, 'F1': F1, 'tol': 0.0001}\n",
    "    print('Done for ', seed)  \n",
    "    with open('results/stan/{}_{}.pkl'.format(filename, seed), 'wb') as f:\n",
    "      pickle.dump(results, f)\n",
    "      \n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# toy example\n",
    "stan_vi(X1, y1, 'vi_toy_example', seeds=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# credit card\n",
    "stan_vi(X, y, 'vi_credit', seeds=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "[2] Edward [tutorial](http://edwardlib.org/tutorials/supervised-regression)\n",
    "\n",
    "[3] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
