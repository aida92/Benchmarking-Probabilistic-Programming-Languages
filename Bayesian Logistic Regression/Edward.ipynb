{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression in Edward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from io import BytesIO\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "import edward as ed\n",
    "from edward.models import Bernoulli, Normal, Empirical\n",
    "\n",
    "from utils import compute_metrics, SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Fraud Detection [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%gcs read --object \"gs://thesis-203306/data/creditcard.csv\" --variable csv_as_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(BytesIO(csv_as_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = np.array(df.Class.tolist())     \n",
    "df = df.drop('Class', 1)\n",
    "\n",
    "df = df.drop('Time', 1)     \n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "X = np.array(df.values)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy Dataset \n",
    "\n",
    "Adapted from [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D=1, noise_std=0.1):    \n",
    "    X = np.concatenate([np.linspace(-6, -5, num=5), np.linspace(2, 6, num=N-5)])\n",
    "    y = np.tanh(X) + np.random.normal(0, noise_std, size=N)\n",
    "    y[y < 0.5] = 0\n",
    "    y[y >= 0.5] = 1\n",
    "    X = (X - 4.0) / 4.0\n",
    "    X = X.reshape((N, D))\n",
    "    return X, y.astype(int)\n",
    "  \n",
    "N1 = 100\n",
    "D1 = 1\n",
    "X1, y1 = build_toy_dataset(N1, D=1, noise_std=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _ed_MH(X, y, iters=2000, burn=1000):\n",
    "  \"\"\"\n",
    "  Runs Edward's Metropolis-Hastings algorithm on one dataset\n",
    "  \"\"\"\n",
    "  # The model\n",
    "  n_samples, n_dim = X.shape\n",
    "  X_e = tf.placeholder(tf.float32, [n_samples, n_dim])\n",
    "  w_e = Normal(loc=tf.zeros(n_dim), scale=3.0 * tf.ones(n_dim))\n",
    "  b_e = Normal(loc=0.0, scale=3.0)\n",
    "  y_e = Bernoulli(logits=ed.dot(X_e, w_e) + b_e)\n",
    "    \n",
    "  qw = Empirical(params=tf.Variable(tf.zeros([iters, n_dim])))\n",
    "  qb = Empirical(params=tf.Variable(tf.zeros([iters])))\n",
    "  \n",
    "  w_proposal = Normal(loc=w_e, scale=0.1)\n",
    "  b_proposal = Normal(loc=b_e, scale=0.1)\n",
    " \n",
    "  inference = ed.MetropolisHastings({w_e: qw, b_e: qb}, {w_e: w_proposal, b_e: b_proposal}, data={X_e: X, y_e: y})\n",
    "    \n",
    "  start = timer()\n",
    "  inference.run()\n",
    "  time = timer() - start\n",
    "    \n",
    "  return qw.params.eval()[:burn], qb.params.eval()[:burn], time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ed_MH(X, y, filename, iters=2000, burn=1000, seeds=SEED):\n",
    "  \"\"\"\n",
    "  Runs Edward's Metropolis-Hastings algorithm\n",
    "  \"\"\"\n",
    "  for seed in seeds:\n",
    "    print(seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    qw, qb, time = _ed_MH(X_train, y_train, iters, burn)\n",
    "    \n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    F1 = []\n",
    "    \n",
    "    for ww, bb in zip(qw, qb):\n",
    "      a, p, r, f = compute_metrics(ww, bb, X_test, y_test)\n",
    "      accuracy.append(a)\n",
    "      precision.append(p)\n",
    "      recall.append(r)\n",
    "      F1.append(f)\n",
    "    \n",
    "    results = {'time': time, 'w': qw, 'b': qb, 'iters': iters, 'burn': burn, \n",
    "              'accuracy': accuracy, 'precision': precision, 'recall': recall, 'F1': F1}\n",
    "    with open('results/edward/{}_{}.pkl'.format(filename, seed), 'wb') as f:\n",
    "      pickle.dump(results, f)\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credit card fraud - short chain\n",
    "ed_MH(X, y, 'mh_credit', iters=2000, burn=1000, seeds=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Credit card fraud - long chain\n",
    "ed_MH(X, y, 'mh_credit_long', iters=8000, burn=4000, seeds=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _ed_KLqp(X,y,N,D,iters):\n",
    "  \"\"\"\n",
    "  Runs Edward's Klqp algorithm on one dataset\n",
    "  \"\"\"\n",
    "  X_e = tf.placeholder(tf.float32, [N, D])\n",
    "  w_e = Normal(loc=tf.zeros(D), scale=3.0 * tf.ones(D))\n",
    "  b_e = Normal(loc=0.0, scale=3.0)\n",
    "  y_e = Bernoulli(logits=ed.dot(X_e, w_e) + b_e)\n",
    "    \n",
    "  with tf.variable_scope('qw_loc', reuse=tf.AUTO_REUSE):\n",
    "    qw_loc = tf.get_variable(\"qw_loc\", [D])\n",
    "  with tf.variable_scope('qw_scale', reuse=tf.AUTO_REUSE):\n",
    "    qw_scale = tf.nn.softplus(tf.get_variable(\"qw_scale\", [D]))\n",
    "  with tf.variable_scope('qb_loc', reuse=tf.AUTO_REUSE):\n",
    "    qb_loc = tf.get_variable(\"qb_loc\", []) + 10.0\n",
    "  with tf.variable_scope('qb_scale', reuse=tf.AUTO_REUSE):\n",
    "    qb_scale = tf.nn.softplus(tf.get_variable(\"qb_scale\", []))\n",
    "\n",
    "  qw = Normal(loc=qw_loc, scale=qw_scale)\n",
    "  qb = Normal(loc=qb_loc, scale=qb_scale)\n",
    "\n",
    "  inference = ed.KLqp({w_e: qw, b_e: qb}, data={X_e: X, y_e: y})\n",
    "  start = timer()\n",
    "  inference.run(n_iter=iters)\n",
    "  end = timer()\n",
    "  \n",
    "  return qw, qb, end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ed_KLqp(X, y, filename, seeds=SEED):\n",
    "  \"\"\"\n",
    "  Runs Edward's KLqp algorithm for each seed\n",
    "  \"\"\"\n",
    "  for seed in seeds:\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed)\n",
    "    N,D = X_train.shape\n",
    "    \n",
    "    iters = np.linspace(500, 10000, 5).astype(int)\n",
    "    \n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    F1 = []\n",
    "    times = []\n",
    "    \n",
    "    for it in iters:\n",
    "      qw, qb, time = _ed_KLqp(X_train,y_train,N,D,it)\n",
    "      w = qw.eval()\n",
    "      b = qb.eval()\n",
    "      a, p, r, f = compute_metrics(w, b, X_test,y_test)\n",
    "      accuracy.append(a)\n",
    "      precision.append(p)\n",
    "      recall.append(r)\n",
    "      F1.append(f)\n",
    "      times.append(time)\n",
    "    \n",
    "    results = {'iters': iters, 'times': times, 'accuracy': accuracy, 'precision': precision,\n",
    "               'recall': recall, 'F1': F1}\n",
    "    with open('results/edward/{}_{}.pkl'.format(filename, seed), 'wb') as f:\n",
    "      pickle.dump(results, f)\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Credit card fraud detection\n",
    "ed_KLqp(X, y, 'vi_credit', seeds=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "[2] Edward [tutorial](http://edwardlib.org/tutorials/supervised-regression)\n",
    "\n",
    "[3] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
