{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets as skd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from utils import SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy dataset \n",
    "\n",
    "Adapted from from [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D = 1, noise_std=0.1):    \n",
    "    X = np.concatenate([np.linspace(-6, -5, num=5), np.linspace(2, 6, num=N-5)])\n",
    "    y = np.tanh(X) + np.random.normal(0, noise_std, size=N)\n",
    "    y[y < 0.5] = 0\n",
    "    y[y >= 0.5] = 1\n",
    "    X = (X - 4.0) / 4.0\n",
    "    X = X.reshape((N, D))\n",
    "    return X, y.astype(int)\n",
    "  \n",
    "N1 = 100\n",
    "D1 = 1\n",
    "X1, y1 = build_toy_dataset(N1, D=1, noise_std=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N2 = 1000\n",
    "D2 = 20\n",
    "X2, y2 = skd.make_classification(n_samples=N2, n_features=D2, n_redundant=5, \n",
    "                           n_informative=15, random_state=7, n_clusters_per_class=1)\n",
    "X2_train,X2_test,y2_train,y2_test = ms.train_test_split(X2,y2,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N3 = 1000\n",
    "D3 = 2\n",
    "X3, y3 = skd.make_moons(n_samples=N3, noise=0.1, random_state=42)\n",
    "X3_train,X3_test,y3_train,y3_test = ms.train_test_split(X3,y3,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N4 = 1000\n",
    "D4 = 2\n",
    "X4, y4 = skd.make_circles(n_samples=N4, noise=0.05, factor=0.3, random_state=0)\n",
    "X4_train,X4_test,y4_train,y4_test = ms.train_test_split(X4,y4,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"data/creditcard.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array(df.Class.tolist())     \n",
    "df = df.drop('Class', 1)\n",
    "df = df.drop('Time', 1)     \n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "\n",
    "X = np.array(df.values)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrn = LogisticRegression()\n",
    "accuracy = []\n",
    "recall = []\n",
    "precision = []\n",
    "F1 = []\n",
    "for seed in SEED:\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "  lrn.fit(X_train, y_train)\n",
    "\n",
    "  y_prob = lrn.predict_proba(X_test)[:,lrn.classes_[1]]\n",
    "  y_pred = np.zeros(len(y_prob))\n",
    "\n",
    "  for i in range(len(y_prob)):\n",
    "      if y_prob[i] > 0.5:\n",
    "          y_pred[i] = 1\n",
    "\n",
    "  total = y_test.shape\n",
    "  TP = np.sum(y_pred * y_test)\n",
    "  FP = np.sum(y_pred - y_pred * y_test)\n",
    "  TN = np.sum((1-y_pred) * (1-y_test))\n",
    "  FN = total - TP - FP - TN\n",
    "    \n",
    "  a = ((TP + TN) / total)[0]\n",
    "  p = (TP / (TP + FP))\n",
    "  r = (TP / (TP + FN))[0]\n",
    "  f = (2 / (1/r + 1/p))\n",
    "  \n",
    "  accuracy.append(a)\n",
    "  precision.append(p)\n",
    "  recall.append(r)\n",
    "  F1.append(f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "\n",
    "[2] Edward [tutorial](http://edwardlib.org/tutorials/supervised-regression)\n",
    "\n",
    "[3] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
